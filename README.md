# genai_video_creater

✅ Overview
Agentic AI Video Creator is an AI-powered system that transforms user-provided text and images into an engaging video, enriched with captions and background music.
It leverages LangGraph for agentic orchestration and MoviePy + FFmpeg for video rendering.

✅ Current Architecture Summary
FastAPI backend with async routes.

Workflow using LangGraph with nodes:

Script Generation → Audio Generation (TTS) → Music Generation & Mix → Video Assembly.

Assets processed locally (pydub, moviepy).

External API calls:

OpenAI for script & TTS.

HuggingFace for music generation.

⚠️ Bottlenecks & Solutions
1. External API Latency & Rate Limits
Problem:

OpenAI TTS and HuggingFace MusicGen can be slow (5–20s per request).

Hitting rate limits (429 errors).

Impact: Delays in audio/music generation → video pipeline blocked.

✅ Solutions:

Batch API Calls:
For long scripts, split text into chunks and send parallel requests (asyncio.gather).

Caching:
Cache responses for same text prompts using Redis/MongoDB.

Fallback Models:

If OpenAI TTS fails → gTTS.

If HuggingFace MusicGen fails → static music track or local AI model.

Queue Processing:
Use Celery or RQ with Redis for job scheduling and retries.

2. Video Rendering Bottleneck (MoviePy)
Problem:

moviepy is CPU-bound, uses single-threaded ffmpeg.

Long videos or many images cause slow final rendering.

✅ Solutions:

Use FFmpeg directly for faster video rendering (multi-thread support).

Parallelize image processing (resizing, duration assignment) using concurrent.futures.

Preprocess images asynchronously before passing to moviepy.

3. Audio Mixing with pydub
Problem:

pydub loads entire audio into memory → RAM-heavy for long audio.

Music looping and overlay is slow.

✅ Solutions:

Use FFmpeg filter_complex for mixing instead of pydub (much faster, stream-based).

Example:

bash
Copy
Edit
ffmpeg -i narration.mp3 -i music.mp3 -filter_complex "[1:a]volume=0.3[a1];[0:a][a1]amix=inputs=2:duration=first" output.mp3
4. Disk I/O & Temporary Files
Problem:

Images, audio, and video stored in local disk → slow read/write if many assets.

✅ Solutions:

Use in-memory processing with BytesIO when possible.

Store temporary files in /tmp (Linux) or RAM-based FS.

For production: upload assets to S3 / GCS and stream.

5. Scaling & Concurrency
Problem:

FastAPI handles async, but heavy CPU work (video rendering) blocks event loop.

✅ Solutions:

Offload CPU-heavy tasks to background workers using Celery/RQ.

Deploy with Gunicorn + Uvicorn workers (--workers N --threads M).

For real scaling → Kubernetes + autoscaling.

6. Model Size & Dependency Overhead
Problem:

Loading HuggingFace MusicGen locally = heavy RAM/GPU.

✅ Solutions:

Use hosted inference API for lighter setups.

For local: load small models like musicgen-small or quantized models.

7. Error Handling & Resilience
Problem:

API errors crash pipeline (e.g., OpenAI 429, HuggingFace down).

✅ Solutions:

Implement retry logic with exponential backoff.

Use fallback paths (e.g., static music if MusicGen fails).

Add structured logging for debugging.

✅ Proposed Scalable Architecture
css
Copy
Edit
Client → FastAPI API → Task Queue (Celery/RQ)
        ↓                          ↓
     Redis/MongoDB            Worker Pods
                                 ↓
          [Script] → [TTS] → [MusicGen] → [Mix] → [FFmpeg Video Render]
                                 ↓
                            Upload to S3/GCS
Async API returns a job_id immediately.

Workers handle heavy lifting.

Status stored in Redis/MongoDB.

Horizontal scaling with more workers for high demand.
